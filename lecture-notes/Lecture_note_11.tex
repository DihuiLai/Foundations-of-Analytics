\documentclass[12pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{listings}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{1.5pt}%

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\block}[1]{
  \underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}
}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\usepackage{tikz}  %TikZ central library is called.
\usetikzlibrary{automata,positioning} 
\usepackage{standalone}
\usepackage{pdfpages}

\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=0.8cm
  },
  neuron missing/.style={
    draw=none, 
    scale=2,
    text height=0.2cm,
    execute at begin node=\color{black}$\vdots$
  },
  arro/.style={
    ->,
    >=latex
  },
  bloque/.style={
    draw,
    minimum height=1cm,
    minimum width=0.5cm
  }  
}


\title{Lecture Note - 10: Document Classification, Support Vector Machine (SVM), Sentiment Analysis}
\author{Dihui Lai}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}


\section{Document Representation}

\subsection{Document Representations}
A document can be represented by the words and the number of their occurrence using term-document matrix
\begin{center}
 \begin{tabular}{c c c c c} 
 \hline
 Words & As You Like It& Twelfth Night& Ulius Caesar& Henry V\\ [0.5ex] 
 \hline\hline
 battle & 1 & 0 & 7 &13 \\ 
 \hline
 good & 114 & 80 &62 & 89\\
 \hline
 fool & 36 & 58 & 1 & 5 \\
 \hline
 wit & 20 & 15 & 2 & 3 \\
 \hline
\end{tabular}
\end{center}



\subsection{tf-idf Weighted Measure}
Certain words are more common in all documents e.g. the, it, they. The less frequent like "litigation" might be more important that the more frequent word "good". The tf-idf algorithms can be used to adjust the intuition.
$$
w_{t,d} = tf_{t,d}\times idf_{t}
$$

Here,
$$
tf_{t, d}=1 + log_{10}(count(t, d)) \text{ if } count(t, d) > 0, \text{ else } 0
$$
With $count(t, d)$  the number of occurence of the term $t$ in the document $d$

$$
idf_{t}=log_{10}\left(\frac{N}{df_t}\right)
$$ 
N is the total number of documents in the collection, and $df_t$ is the number of documents in which term t occurs. If word good appears in all collected document, $idf_t=0$


\subsection{Document/Paragraph to Vector Representation}

\begin{itemize}
\item Average of the word embedding: take the mean of the vector embedding of the words in the document. 
\item LDA (Latent Dirichlet Allocation) [David Blei, Andrew Ng, and Michael I. Jordan, 2003]
\item Distributed Memory version of Paragraph Vector (PV-DM)
\item ...
\end{itemize}




\section{Document Classification, Sentiment Analysis}
\begin{itemize}
\item Document classification: use machine learning to classify a document type, novel, news, entertainment etc. 
\item Sentiment Analysis: given a document/paragraph, infer the underlying sentiment (positive/negative, like/dislike)
\end{itemize}



\section{Support Vector Machine (SVM)}

\begin{itemize}
\item SVM is a classification/regression model that use hyperplane to categorize data points in high dimension space. 
\item Loss function: $\mathnormal{L=\frac{1}{2}|w|^2}$ when $\mathnormal{y^i (\vec{w}\cdot \vec{x}^i+b)\geq 1}$
\item or equivalently  $\mathnormal{L'=\frac{1}{2}|w|^2-\sum\limits_{i=1}^N \alpha_i (y^i (\vec{w}\cdot \vec{x}^i+b)-1)}$ 
\item Optimization: $\mathnormal{\frac{\partial L'}{\partial w_i}=0}$
\end{itemize}




\section{Other NLP Challenges and Transfer Learning}

\begin{itemize}
\item Question and answer, e.g. SQuAD (the Stanford Question Answering Dataset)
\item Machine translation
\item Multi-lingual NLP
\item Transfer learning e.g. BERT (\url {https://arxiv.org/abs/1810.04805})
\end{itemize}



%% normal frame
\section{Regular Expression}


\begin{itemize}
\item Algebraic notation for characterizing a set of strings.
\item Useful for searching in corpus of texts
\item Python example: str = "The rain in Spain"; x = re.findall("Spain", str)
\end{itemize}





\begin{itemize}
\item[-][]: Used to indicate a set of characters e.g. [abc] will match 'a', 'b', or 'c'.
\item[-]'\textbackslash d': matches any decimal digit; equivalent to the set [0-9].
\item[-] '+': match 1 or more repetitions of the preceding RE
\item[-]'\textbackslash D': anything but a number (a non-digit)	
\item[-] '\textbackslash s': space (tab,space,newline etc.)
\item[-] '\textbackslash S': anything but a space e.g. '\textbackslash S+@\textbackslash S+' represents email address
\item[-]'\textasciicircum': This expression matches the start of a string
\end{itemize}



\subsection{Expression in Python}
re.search(regex, text) returns a match object when the pattern is found or not match if the pattern is not found.
\begin{lstlisting}
import re
m = re.search('hello', 'hello world, hello all, good afternoon')
print m.group(0)
#hello
\end{lstlisting}

re.findall(regex, text) will return a list of all the matches.
\begin{lstlisting}
m = re.findall('hello', 'hello world, hello all, good afternoon')
print m
#['hello', 'hello']
\end{lstlisting}


\end{document}

