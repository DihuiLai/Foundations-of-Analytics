\documentclass[12pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}
\usepackage{mathtools}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{1.5pt}%

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\block}[1]{
  \underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}
}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\usepackage{tikz}  %TikZ central library is called.
\usetikzlibrary{automata,positioning} 
\usepackage{standalone}
\usepackage{pdfpages}

\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=0.8cm
  },
  neuron missing/.style={
    draw=none, 
    scale=2,
    text height=0.2cm,
    execute at begin node=\color{black}$\vdots$
  },
  arro/.style={
    ->,
    >=latex
  },
  bloque/.style={
    draw,
    minimum height=1cm,
    minimum width=0.5cm
  }  
}


\title{Lecture Note - 10: POS Tagging, HMM, NER}
\author{Dihui Lai}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}

\section{Part-of-Speech (POS) Tagging}
As you perhaps learned in grammer class that a word could be one the following types: noun, verb, article, adjective, preposition, pronoun, adverb etc. These categories are called part of speech tags. We can certainly have a refined version of the tags. An important tagset for English is the 45-tag Use \href{https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}{\underline{Penn Treebank tagset}}. Here is one example of POS tag for a sentence using Penn Treebank:

The$[DT]$ Itek$[NNP]$ Air$[NNP]$ Boeing$[NNP]$ 737$[CD]$ took$[VBD]$ off$[RP]$ bound$[VBN]$ for$[IN]$ Mashhad$[NNP]$ in$[IN]$ north-eastern$[JJ]$ Iran$[NNP]$.

How can we get a program to understand the POS tag of a word in a sentence? A most straight forward way is to calculate the probability using the following algorithm

If a word $w$ that could be tagged as $\mathnormal{t_1}$, $\mathnormal{t_2}$, ... $\mathnormal{t_k}$, the probability that the word has tag $\mathnormal{t_i}$ is
\begin{equation*}
{p(t_i|w) = \frac{c(w,t_i)}{\sum\limits_{i=1}^{k}c(w, t_i)}}
\end{equation*}

However, this approach does not take the context of the word into consideration, which is very important to POS tagging. For example the word \textit{account} could be a noun or verb depending on the context: 

A number of factors \textit{account} [VERB] for the differences between the two scores.

How do I open an \textit{account} [NOUN] with your bank?


\section{Hidden Markove Model}

Provided that we have a sequence of words $\mathnormal{W=w_1, w_2,  ...w_i, ...w_n}$ and we want to figure out the corresponding POS tag sequence $\mathnormal{T= t_1, t_2, ... t_i ...t_n}$

Using Bayes' theorem, the conditional probability of having a POS tag sequence T given a word sequence W is 
\begin{equation}
\mathnormal{P(T | W) = P(W|T) P(T) / P(W) = const \times P(W|T) P(T)}
\end{equation}

Generally, we have 
\begin{align*}
P(T)&= P(t_1) P(t_2 | t_1) P(t_3 | t_1, t_2) P(t_4 | t_1, t_2, t_3)  ...P(t_n | t_{1}, t_{2}, ... t_{n-1})
\end{align*}
Assume that ${t_i}$ is only dependent on ${t_{i-1}}$ (Markov assumption) we have 
\begin{equation}
P(T)=P(t_1) P(t_2 | t_1) P(t_3 | t_2) P(t_4 |  t_3)  ...P(t_n | t_{n-1})
\end{equation}
The conditional probability $P(t_i|t_{i-1})$ is called transition probability. It describe the probability that a tag $t_i$ appears after a tag $t_{i-1}$. To calculate the emission probability we can use the following equation
\begin{equation}
\mathnormal{P(t_{i}|t_{i-1}) = \frac{c(t_{i-1},t_{i})}{c(t_{i-1})}} \text{ (transition probability)}
\end{equation}
Where $c(t_{i-1},t_{i})$ is the frequency that tag $t_i$ occurs next to $t_{i-1}$ and $c(t_{i})$ is the frequency that tag $t_i$ occurs in a document.

To calculate the term $P(W|T)$ in equation, we assume that $w_i$ is only dependent on $t_i$ and  
\begin{equation}
P(W|T) = P(w_1 | t_1) P(w_2 | t_2) P(w_3 | t_3)  ...P(w_n | t_{n})
\end{equation}

The condtional probabily $P(w_i|t_i)$ is called emission probability, and can be calculated as 

\begin{equation}
\mathnormal{P(w_i|t_i) = \frac{c(w_i,t_i)}{c(t_i)}} \text{ (emission probability)}
\end{equation}

Where $c(w_i, t_i)$ is the frequency that a word $w_i$ is tagged as $t_i$ or that a tag $t_i$ is used for word $w_i$. In the denominator, $c(t_i)$ is the frequency that the tag $t_i$ occurs in a document.

In summary, we have 
\begin{equation}
{P(T|W) \sim P(t_1) P(t_2|t_1) ... P(t_{n}|t_{n-1}) P(w_1|t_1) P(w_2|t_2) ... P(w_n|t_n)}
\end{equation}

The best tag sequence is the sequence that maximize the conditional probability $P(T|W)$ i.e.
\begin{align*}
\mathnormal{(t_1, t_2, ...t_n)} & = \mathnormal{\argmax_{t_1, t_2, ...t_n}P(t_1, t_2..., t_n|w_1, w_2, ...w_n)}\\
&= \mathnormal{\argmax_{t_1, t_2, ...t_n} \prod\limits_{i=1}^n P(w_i |t_i ) P(t_i |t_{i-1})}
\end{align*}

To solve the problem, we use an algorithm called Viterbi Algorithms. 

\section{Named Entity Recognition}
POS tag helps understanding document at a single word level. Yet we often need to go beyond single word level to understand the semantics of a document. When read independently, the word \textit{white} and \textit{house} means color and the place where people live. When put together, \textit{white house} means a special place where the American president live. In order to understand thw words' meaning as a group. We use an algorithm called named entity recogition(NER). NER labels word chucks in a corpus that are names of things e.g. person, organization, money amount, gene and proteins etc. 

For exmpale, {Alex$[PERSON]$ is going to Los $[LOCATION]$ Angeles $[LOCATION]$}

One chanllenge of NER tagging is that not all words belong to an entity. In the example abve, words \textit{is going to} do not belong to any entity. On the other hand, Los Angeles belong to the same entity but we are not reflecting this fact by tagging them separately. 

A better way of tagging the entiteis properly is using a tagging method called IOB $[\textit{I-Inside}]$, $[\textit{O-Outside}]$, $[\textit{B-Begin}]$.

For the example above, we tag the words as below
\begin{align*}
&{Alex} [\textit{I-PER}]\\
&\textit{is} [\textit{O}]\\
&\textit{going} [\textit{O}]\\
&\textit{to} [\textit{O}]\\
&\textit{Los} [\textit{B-LOC}]\\
&\textit{Angeles} [\textit{I-LOC}]
\end{align*}

\end{document}

