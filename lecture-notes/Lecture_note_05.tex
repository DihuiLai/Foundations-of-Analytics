\documentclass[12pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}
\usepackage{hyperref}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{1.5pt}%

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\block}[1]{
  \underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}
}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}


\title{Lecture Note - 05: Exponential Family, Generalize Linear Model (GLM)}
\author{Dihui Lai}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}

\section{Exponential Family }

\subsection{Probability Density Function}
The probability density function (PDF) of an exponential family can be written in a unified format
$$
{f(y)}=\exp{\left( \frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}
$$

There are two parameters in the function: $\theta$ and $\phi$. Probability distributions like normal, Poisson or binomial can all be written in this format when we choose the right functions $a(\cdot)$, $b(\cdot)$, and $c(y, \cdot)$ and parameter $\theta$,  $\phi$. 

\subsection{Special Case: Gaussian Distribution}
For example, if we set $\theta$ as the mean and $\phi$ as the standard deviation:
$$\theta=\mu$$
$$\phi=\sigma$$
Make the three functions $a$, $b$, $c$, to be the followings:
$$a(\phi)=\phi^2=\sigma^2$$
$$b(\theta)=\frac{\theta^2}{2}=\frac{\mu^2}{2}$$
$$c(y,\phi)=-\frac{y^2}{2\phi^2}-log(\sqrt{2\pi}\phi)=-\frac{y^2}{2\sigma^2}-log(\sqrt{2\pi}\sigma)$$

Substitute the functions into f(y), we have 
\begin{align*}
{f(y)}
&=\exp{\left( \frac{y\mu-\frac{\mu^2}{2}}{\sigma^2}-\frac{y^2}{2\sigma^2}-\log(\sqrt{2\pi}\sigma)\right)}\\
&=\exp{\left( \frac{2y\mu-\frac{2\mu^2}{2}-y^2}{2\sigma^2}-\log(\sqrt{2\pi}\sigma)\right)}\\
&=\exp{\left( \frac{-(y-\mu)^2}{2\sigma^2}\right)\exp(-\log(\sqrt{2\pi}\sigma))}\\
&=\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( \frac{-(y-\mu)^2}{2\sigma^2}\right)}
\end{align*}
This is exactly the Gaussian distribution with mean $\mu$ and standard deviation $\sigma$

\subsection{Special Case: Bernoulli Distribution}
Alternatively, if we set the parameters and the functions to be the followings
\begin{align*}
&\theta=\log(p)-\log(1-p)\\
&a(\phi)=1\\
&b(\theta)=\log(1+e^\theta)=-\log(1-p)\\
&c(y,\phi)=0
\end{align*}

We have the PDF function as
\begin{align*}
{f(y)}&=\exp{\left( \frac{y(\log(p)-\log(1-p))-(-\log(1-p))}{1}+0\right)}\\
&=\exp{\left(\log(p^y)+\log(1-p)^{(-y)})+\log(1-p)^{1}\right)}\\
&=\exp{\left(\log(p^y)+\log(1-p)^{(1-y)})\right)}\\
&=p^y(1-p)^{(1-y)}\\
&=\begin{cases}
p \text{,  } &y=1\\
1-p \text{,  } &y=0
\end{cases}
\end{align*}
, which is exactly the probability mass of a Bernoulli distribution.

\section{Moments of Exponential Family}

\subsection{Mean}
If a random variable $Y$'s PDF belongs to the exponential family. We have the mean and variance of Y immediately as
\begin{align}
&E(Y)=\frac{d}{d\theta}b(\theta)\\
&Var(Y)=a(\phi)\frac{d^2}{d\theta^2}b(\theta)
\end{align}

To derive the equations above, we can simply use the condition that $f(y)$ has to satisfy
$$\int_{-\infty}^{\infty}f(y)dy=1$$ i.e.
\begin{equation}
\int_{-\infty}^{\infty}\exp{\left( \frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=1
\end{equation}


Take the derivative against $\theta$ on both side of the equation, we have

$$\frac{d}{d\theta}\int_{-\infty}^{\infty}\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=0$$
Without proof, we assume the derivative and integration are interchangeable. We then get
$$\int_{-\infty}^{\infty}\frac{d}{d\theta}\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=0$$
The the derivative against the exponential function, we have 
$$\int_{-\infty}^{\infty}(y-b'(\theta))\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=0$$ 
Rearrange the equation, we have 
$$\int_{-\infty}^{\infty}y\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=b'(\theta)\int_{-\infty}^{\infty}\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy$$
This is pretty nice. Because the left hand side is simply the definition of the expectation of y. On the right side, we can simplify the term using equation (3). We therefore end up having
$$E(Y)=\int_{-\infty}^{\infty}y\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=b'(\theta)1=b'(\theta)$$

\subsection{Variance}
To get the variance of $Y$, we can use the same trick of taking derivative but against equation (1) or its equivalence

$$\int_{-\infty}^{\infty}y\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=b'(\theta)$$

i.e.

$$\frac{d}{d\theta}\int_{-\infty}^{\infty}y\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=b''(\theta)$$

Again, assuming the derivative and integration is interchangeable, we have

$$\int_{-\infty}^{\infty}\frac{y(y-b'(\theta))}{a(\phi)}\exp{\left(\frac{y\theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)}dy=b''(\theta)$$

We therefore have 
$$E(Y^2)-(b'(\theta))^2=a(\phi)b''(\theta)$$ 
or equivalently
$$Var(Y)=E(Y^2)-E(Y)^2=a(\phi)b''(\theta)$$ 

\section{Generalized Linear Model (GLM)}
In logistic regression, the model assume the observed outcomes or values of the target variable follow binomial distribution with  parameter $p$. The parameter $p$ or the expectation of the distribution is modeled as a logistic function of a series of predictors $\vec{x}$ i.e. $p=\frac{1}{1+\exp(-\vec{\beta}\cdot\vec{x})}$. 

\subsection{Link Function}
In general, our target variable could follow different types of distribution e.g. Poisson, Gaussian etc. The generalized linear model can be used when the target variable y follows a distribution that belongs to the exponential family. 

The expectation of Y is modeled using a link function $g^{-1}(\cdot)$ 
$$E(Y)=b'(\theta)=g(\vec{x}\cdot\vec{\beta})$$
For simplicity, we denote $\eta=\vec{x}\cdot\vec{\beta}$ and thus 
\begin{equation}
E(Y)=b'(\theta)=g(\eta)
\end{equation}


If we want to estimate the parameter $\theta$, we can solve the equation (4) by taking the inverse of the function $b'(\cdot)$. If we choose the inverse link function $g(\cdot)$ to be the same as $b'(\cdot)$
\begin{equation}
g(\cdot)=b'(\cdot)
\end{equation}
we get a model with nice property where $\eta=\theta$. A link function chosen this way is called a canonical link function.

\subsection{Canonical Link Function}
\textbf{Bernoulli Distribution} Use Bernoulli distribution as an example, we have $b'(\theta)=\frac{e^\theta}{1+e^\theta}$. The inverse canonical link function is 

$$g(\eta)=\frac{e^{\eta}}{1+e^{\eta}}=\frac{1}{1+e^{-\eta}}$$
As we known,  $$p=b'(\theta)=\frac{1}{1+e^{-\eta}}$$ The equation looks a lot like the probability model in the logistic regression model. In fact, as we will learn in the following section, GLM in the special case of a Bernoulli distribution is equivalent to a logistic regression.

\textbf{Gaussian Distribution}: in the case of a Gaussian distribution, $b'(\theta)=\theta$, which is an identity function. Choose the link function as the identify function, we have 
$$g(\eta)=\eta$$
Since we have $\mu=\theta$, the canonical link function of a Gaussian distribution models the expectation of the target variable as of the following
$$\mu=\eta=\vec{x}\cdot\vec{\beta}$$ 

This looks a lot like the linear regression model, except that we don not know how GLM estimates the error function yet. In the following sections, we are going to discuss how to use MLE to optimize the a GLM.

\subsection{Maximum Likelihood Estimation}

\subsubsection{Likelihood Function}
A good model represent a dataset as close as it can.
We can uses maximum likelihood estimation to optimize the $\vec{\beta}$ in GLM. 

Considering one data entry ($\vec{x}^i$, $y^i$), the corresponding log-likelihood function is $$\ell^i=\frac{y^i\theta^i-b(\theta^i)}{a(\phi)}+c^i(y^i, \phi)$$

Since only $\theta^i$ is dependent on $\vec{\beta}$, to maximize the function is equivalent to minimize the function $\ell^i=-2 \left[y^i\theta^i-b(\theta^i)\right]$. The log-likelihood for the whole data set is a summation of each individual $\ell^i$. Overall, we need to minimize the following log-likelihood.

\begin{equation}
\ell=-2\sum_{i=1}^{N} \left[y^i\theta^i-b(\theta^i)\right]
\end{equation}

In the case of Bernoulli distribution we have 
$\ell=y^i\theta^i-\log\left(1+\exp(\theta^i)\right)$

\subsubsection{Gradient}
To maximize the likelihood $\ell$, we need to have
$$\frac{\partial \ell}{\partial \beta^j}=0$$ 
By using equation (6), we end up having
\begin{align}
\frac{\partial \ell}{\partial \beta^j}&=-2\sum_{i=1}^{N}\frac{\partial }{\partial \beta^j} \left[y^i\theta^i-b(\theta^i)\right]\\
&=-2\sum_{i=1}^{N}\left[y^i-b'(\theta^i)\right]\frac{\partial \theta^i}{\partial\beta^j}=0
\end{align}

If we use canonical link function, we know $\eta=\theta$, the equation can be simplified as below
\begin{align}
\frac{\partial \ell}{\partial \beta^j}&=-2\sum_{i=1}^{N}\left[y^i-b'(\theta^i)\right]\frac{\partial \eta^i}{\partial\beta^j}\\
&=-2\sum_{i=1}^{N}\left[y^i-b'(\theta^i)\right]x^i_j=0
\end{align}

In the case of a Bernoulli distribution, we have $b'(\theta)=p$

\begin{align*}
\frac{\partial \ell}{\partial \beta^j}=\sum\limits_{i=1}^N[y^ix^i_j-p^ix^i_j]
\end{align*}
This is equivalent to the corresponding equation using MLE in logistic regression. In another word, GLM is equivalent to logistic regression when we assume Bernoulli/binomial distribution of the target variable and use canonical link function.

In general, $\eta\neq\theta$ and we will not have an estimator as simple as equation (10). We need to solve equation (8) instead. In order to do this numerically, we need to figure out the value of $\frac{\partial \theta^i}{\partial\beta^j}$ using $\beta$s and $x$s, or $\eta$ equivalently. 

In order to get the relationship between $\theta$ and $\beta$, we start from the equation (4), where the relationship of the two variables are defined by GLM. By taking the derivative against $\beta$s, we have 
\begin{align}
\frac{\partial}{\partial\beta_j}b'(\theta)&=g'(\eta)\frac{\partial \eta}{\partial \beta}\\
b''(\theta)\frac{\partial \theta}{\partial\beta}&=g'(\eta)\frac{\partial \eta}{\partial \beta_j}
\end{align}

Therefore we get 
\begin{equation}
\frac{\partial \theta}{\partial\beta}=\frac{g'(\eta)}{b''(\theta)}\frac{\partial \eta}{\partial \beta_j}
\end{equation}

Inserting this back to equation (8), we have 

\begin{equation}
\frac{\partial \ell}{\partial \beta_j}=-2\sum_{i=1}^{N}\left[y^i-b'(\theta^i)\right]\frac{g'(\eta^i)}{b''(\theta^i)}x^i_j=0
\end{equation}

Notice that we have an extra weight term $\frac{g'(\eta)}{b''(\theta)}$ comparing to equation (10). To simplify the notation, we denote $b'(\theta)=\mu$ and $b''(\theta)=V(\mu)$. We therefore have 
\begin{equation}
\frac{\partial \ell}{\partial \beta_j}=-2\sum_{i=1}^{N}(y^i-\mu^i)\frac{g'(\eta^i)}{V(\mu^i)}x^i_j=0
\end{equation}
\subsubsection{Hessian}
We can use Newton-Raphson method to solve the MLE problem formulated above, however, we need to calculate the Hessian first. 

The Hessian matrix can be estimated by taking the derivative of the gradient, noting here $\mu$ is a function of $\theta$ and therefore dependent on $\beta$.
\begin{align}
\frac{\partial^2 \ell}{\partial\beta_k\partial \beta_j}&=-2\sum_{i=1}^{N}\frac{\partial}{\partial \beta_k}\left[(y^i-\mu^i)\frac{g'(\eta^i)}{V(\mu^i)}x^i_j\right]\\
&=-2\sum_{i=1}^{N}\left[-\frac{\partial \mu^i}{\partial \beta_k}\frac{g'(\eta^i)}{V(\mu^i)}
+(y^i-\mu^i)\frac{g''(\eta^i)V(\mu^i)\frac{\partial \eta^i}{\partial \beta_k}-g'(\eta^i)V'(\mu^i)\frac{\partial \mu^i}{\partial \beta_k}}{V(\mu^i)^2}\right]x^i_j\\ 
&=2\sum_{i=1}^{N}\left[g'(\eta^i)\frac{g'(\eta^i)}{V(\mu^i)}
-(y^i-\mu^i)\frac{g''(\eta^i)V(\mu^i)-g'(\eta^i)^2V'(\mu^i)}{V(\mu^i)^2}\right]\frac{\partial \eta^i}{\partial \beta_k}x^i_j\\
&=2\sum_{i=1}^{N}\left[g'(\eta^i)\frac{g'(\eta^i)}{V(\mu^i)}
-(y^i-\mu^i)\frac{g''(\eta^i)V(\mu^i)-g'(\eta^i)^2V'(\mu^i)}{V(\mu^i)^2}\right]x^i_k x^i_j
\end{align}
Going from equation (17) to (18), we have used the fact that $\frac{\partial\mu}{\partial\beta_j}=\frac{\partial}{\partial\beta_j}b'(\theta)=g'(\eta)\frac{\partial \eta}{\partial \beta}$


\end{document}

