\documentclass[12pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}
\usepackage{hyperref}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{1.5pt}%

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\block}[1]{
  \underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}
}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\title{Multi-class classification Regression Model}
\author{Dihui Lai}

\begin{document}
\maketitle
\tableofcontents

\vspace{.25in}

\section{Likelihood function of Multinomial distribution}

\subsection{Multinomial Distribution}

The multinomial distribution has density function
$${f(y_1, y_2, y_3, ... y_c)=\frac{N!}{y_1! y_2! ... y_c!} p_1^{y_1} p_2^{y_2} ... p_c^{y_c}}$$
where $c$ is the number of classes that is observed in the dataset. For example, if you have 3 types of outcomes then c=3. $N$ is the total number of observations that we have in the dataset. $y_j$, $j=1, 2, ..., c$ is the number of observations that belong to the the $jth$ categories. Accordingly $\sum\limits_{j=1}^c y_j=N$

When we perform $ith$ experiemnt we have $N=1$, the likelihood function of the observation is accordingly, 
$\mathnormal{L_i= \prod \limits_{j=1}^c p_{j}^{y^i_j}}$, note that only one of the $y^i_j$ will be 1 the rest will all be 0, as  $\sum\limits_{j=1}^c y^i_j=1$

The likelihood function for a dataset of size N is then 
$\mathnormal{L= \prod \limits_{i=1}^N\prod \limits_{j=1}^c p_{j}^{y^i_j}}$. Note $y_j^i$ is either 1 or 0.

The log-likelihood function is
\begin{equation}
\mathnormal{\ell=\sum\limits_{i=1}^n \sum\limits_{j=1}^c y_j^i \log(p_j)}
\end{equation}


The reverse of the log-likelihood is also known as the log{-}loss function
$$logloss=\log(L)=-\sum\limits_{i=1}^n \sum\limits_{j=1}^c y_j^i \log(p_j)$$

Reference: \url{https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html}

\subsection{Maximum Likelihood Estimation of $p_j$}
The $p_k$ that maximize the log-likelihood function that subject to the constraint $\sum\limits_{j=1}^c p_j=1$ has to satisfy the following condition (using method of Lagrange multipliers)
\begin{align*}
\begin{cases}
\dfrac{\partial}{\partial p_k} \left(\ell-\lambda \sum\limits_{i=1}^n(1-\sum\limits_{j=1}^c p_j)\right)=0\\
\dfrac{\partial}{\partial \lambda} \left(\ell-\lambda \sum\limits_{i=1}^n(1-\sum\limits_{j=1}^c p_j)\right)=0
\end{cases}
\end{align*}
 
\begin{align*}
\Rightarrow
\begin{cases}
\sum\limits_{i=1}^n\dfrac{\partial}{\partial p_k} \left(\sum\limits_{j=1}^c x_j^i \log(p_j)-\lambda (1-\sum\limits_{j=1}^c p_j)\right)=0\\
\sum\limits_{i=1}^n\dfrac{\partial}{\partial \lambda} \left(\sum\limits_{j=1}^c x_j^i \log(p_j)-\lambda (1-\sum\limits_{j=1}^c p_j)\right)=0
\end{cases}
\end{align*}


\begin{align*}
\Rightarrow
\lambda=-\frac{y_k}{p_k}
\end{align*}

where $x_k$ is the total number of outcome that belong to category $k$. Because
\begin{equation*}
\sum\limits_{k=1}^c y_k=N
\end{equation*}

We have 
\begin{equation*}
-\sum\limits_{k=1}^c\lambda p_k=N \Rightarrow \lambda=-N
\end{equation*}

Therefore
\begin{equation*}
p_k=\frac{y_k}{N}
\end{equation*}


\subsection{Modeling $p_j$ and Softmax }
A reasonable modeling methods for $p_j$ in equation (1) is to use a softmax transformation of the lienar core $\vec{\beta}\cdot\vec{x}$, where $\vec{x}$ is the vector composed of the predictors.

For a given data point $i$, the probability that the outcome being $j$, $j=1, 2, 3, ..., c$ is

\begin{equation}
p_j=\frac{\exp{(\vec{\beta_j}\cdot\vec{x}^i)}}{\sum\limits_{j=1}^c\exp{(\vec{\beta_j}\cdot\vec{x}^i)}}
\end{equation}

Note that we have total c vector $\beta$s, where each $\vec{\beta}_j$, $j=1,2, 3, ...c$ belongs to one of the $c$ possible outcomes. For example, if you have 3 possible outcomes, you will have to estimate 3 $\vec{\beta}$s i.e. $\vec{\beta_1}$, $\vec{\beta_2}$ and $\vec{\beta_3}$.  

In a special case where you have $c=2$, equation 2 can be reduced to 

\begin{equation}
p_1=\frac{\exp{(\vec{\beta_1}\cdot\vec{x}^i)}}{\exp{(\vec{\beta_1}\cdot\vec{x}^i)}+\exp{(\vec{\beta_2}\cdot\vec{x}^i)}}=\frac{1}{1+\exp{(\vec{\beta_2}-\vec{\beta_1})\cdot\vec{x}^i}}
\end{equation}

, which is equivanet to a logistic function.

  

\subsection{Optimization}

The MLE estimation can be accomplished using Newton-Raphson method

\end{document}