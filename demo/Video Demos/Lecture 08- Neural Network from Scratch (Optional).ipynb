{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 07 - Neural Network: Forward, Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z, actype='sigmoid'):\n",
    "    if actype=='sigmoid':\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dactivation(z, actype='sigmoid'):\n",
    "    if actype=='sigmoid':\n",
    "        return np.multiply(activation(z, actype),1-activation(z, actype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification cost function\n",
    "\\begin{equation}\n",
    "p_i=\\frac{\\exp(z_i)}{\\sum\\limits_{k=1}^{K}\\exp(z_k)}\n",
    "\\end{equation}\n",
    "\n",
    "The cost function for one data point\n",
    "\\begin{equation}\n",
    "C=-\\sum\\limits_{j=1}^K y_j\\log p_j=-\\sum\\limits_{j=1}^K \\left(y_j z_j- y_j \\log(\\sum\\limits_{k=1}^K \\exp(z_k))\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial z_i}&=-\\sum\\limits_{j=1}^K \\left(y_j\\delta_{ij}-y_j \\frac{\\exp(z_i)}{\\sum\\limits_{k=1}^K \\exp(z_k)} \\right)\\\\\n",
    "&=-\\sum\\limits_{j=1}^K \\left(y_j\\delta_{ij}-y_j p_i \\right)\\\\\n",
    "&=-(y_i-p_i)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnet:\n",
    "    def __init__(self, inputsize, hiddensize=[128, 10], outputsize=2, bias=True):\n",
    "        \n",
    "        if bias:\n",
    "            inputsize+=1\n",
    "            hiddensize=list(map(lambda x: x+1, hiddensize))\n",
    "        self.weights = []\n",
    "#        self.outputs=np.ones([1, outputsize])\n",
    "#        self.inputs=np.ones([1, inputsize])\n",
    "        self.layersize=[inputsize]+hiddensize+[outputsize]\n",
    "        self.L=len(self.layersize)\n",
    "        # define weight matrix\n",
    "        for i, isize in enumerate(self.layersize):\n",
    "            if i<(self.L-1):\n",
    "                self.weights.append(np.random.randn(isize, self.layersize[i+1]))\n",
    "        self.z=[None]*(self.L-1)\n",
    "        self.a=[None]*(self.L-1)\n",
    "        self.delta=[None]*(self.L-1)\n",
    "        self.dweights=[None]*(self.L-1)\n",
    "\n",
    "    def forward(self):\n",
    "        for i, w in enumerate(self.weights):\n",
    "            \n",
    "            if i==0:\n",
    "                self.z[i] = np.matmul(self.inputs, w) # Calculate the Z for each layer\n",
    "                self.a[i] = activation(self.z[i])     #Calculate the Z for each layer\n",
    "                self.a[i][0,-1] = 1\n",
    "            else:\n",
    "                self.z[i] = np.matmul(self.a[i-1], w)       \n",
    "                self.a[i] = activation(self.z[i])       \n",
    "                self.a[i][0,-1] = 1\n",
    "        \n",
    "    def backprop(self):\n",
    "        self.dactdz=list(map(dactivation, self.z))      \n",
    "        for i, value in enumerate(self.delta):\n",
    "            irev=self.L-2-i\n",
    "            self.dactdz[irev][-1]=1\n",
    "            if i==0:\n",
    "                self.delta[irev]=softmax(self.z[-1])-self.outputs\n",
    "            else:\n",
    "                self.delta[irev]=np.multiply(np.matmul(self.weights[irev+1], self.delta[irev+1].T).T, self.dactdz[irev])\n",
    "        for i, value in enumerate(self.dweights):\n",
    "            if i==0:\n",
    "                self.dweights[i]=np.matmul(self.inputs.T, self.delta[i])\n",
    "            else:\n",
    "                self.dweights[i]=np.matmul(self.a[i-1].T, self.delta[i])\n",
    "\n",
    "    def createbatch(self):\n",
    "            for i in range(0, len(self.data), self.batchsize):\n",
    "                yield self.data[i:i + self.batchsize]\n",
    "    \n",
    "    def fit(self, data, batchsize, learningrate, epochs):\n",
    "        self.batchsize = batchsize\n",
    "        self.data = data\n",
    "        self.batches= list(self.createbatch())\n",
    "        for i in np.arange(epochs):\n",
    "            for minibatch in self.batches:\n",
    "                self.totalDW=[None]*(self.L-1)\n",
    "                for i, value in enumerate(self.totalDW):\n",
    "                    self.totalDW[i]=np.zeros([self.layersize[i], self.layersize[i+1]]) #initialize self.totalDW\n",
    "                for xentry, yentry in minibatch:\n",
    "                    self.outputs=yentry\n",
    "                    self.inputs=np.column_stack([xentry, [1]])\n",
    "                    self.forward()\n",
    "                    self.backprop()\n",
    "                    self.totalDW=list(map(lambda x, y: x+y, self.totalDW, self.dweights))\n",
    "                self.weights=list(map(lambda w, dw: w-dw*learningrate/batchsize,  self.weights, self.totalDW))\n",
    "    def predict(self, x):\n",
    "        self.predz=[None]*(self.L-1)\n",
    "        self.preda=[None]*(self.L-1)\n",
    "        x=np.column_stack([x, [1]])\n",
    "        for i, w in enumerate(self.weights):\n",
    "            if i==0:\n",
    "                self.predz[i] = np.matmul(x, w)\n",
    "                self.preda[i] = activation(self.predz[i])\n",
    "            else:\n",
    "                self.predz[i] = np.matmul(self.preda[i-1], w)       \n",
    "                self.preda[i] = activation(self.predz[i])\n",
    "        return(softmax(self.predz[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = 2*np.random.randn(5000, 1)\n",
    "X2 = 5*np.random.randn(5000, 1)\n",
    "X3 = np.random.randn(5000, 1)\n",
    "eta=0.5*X1+0.1*X2+1.56*X3-1\n",
    "X=np.column_stack([X1,X2, X3])\n",
    "p=1/(1+np.exp(-eta))\n",
    "y=np.random.binomial(1, p).reshape(5000,1)\n",
    "y=np.column_stack([y, 1-y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsticdata=[]\n",
    "for predictor, target in zip(X, y):\n",
    "    logsticdata.append((predictor.reshape(1, -1), target.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.9787409 , -6.2467915 ,  0.00670936,  1.        ]])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.column_stack([logsticdata[0][0], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleneuron=neuralnet(3, [3], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleneuron.fit(logsticdata, 100, learningrate=0.01, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3.21478102,  1.92365079,  0.05862402, -4.7342825 ],\n",
       "        [ 1.36748184, -0.21825899,  0.12854786, -1.68612637],\n",
       "        [ 3.64512275,  3.15857517,  2.4431216 , -6.75217596],\n",
       "        [-0.2272826 ,  0.66779496, -0.78119702,  0.94754739]]),\n",
       " array([[ 0.5166483 , -0.80406116],\n",
       "        [ 0.6493974 , -0.54891423],\n",
       "        [ 1.09190037,  0.02686781],\n",
       "        [-1.41973747,  1.10846792]])]"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleneuron.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09825398, 0.90174602]])"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleneuron.predict(logsticdata[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=[]\n",
    "for ientry in logsticdata:\n",
    "    ypred.append((singleneuron.predict(ientry[0])[0][0]>0.5)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2182, 1115],\n",
       "       [ 329, 1374]])"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(list(y[:,0]), ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2446,  851],\n",
       "       [ 347, 1356]])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(list(y[:,0]), ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.395813866018872"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleneuron.predict(ientry[0])[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.82698596, -7.36548003,  0.05225958]]), array([[0, 1]]))"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsticdata[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07119237])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.22275634, -4.08002901]])]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleneuron.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
